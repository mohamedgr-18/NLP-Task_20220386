{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d909b3d-9619-402b-90f2-96320f96d510",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using RNN with Amazon Product Reviews dataset which has around 25000 customer reviews\n",
    "\n",
    "### Step 1: Importing necessary Libraries . Note : \"Tensorflow require python version between 3.7 to 3.10\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfc1fb-0415-4154-a5a9-218adab6000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea7e5c-8a7c-4877-ac26-d6e0cb4818d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066a79b-60b0-4ac2-92cf-97ff4d43a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe8158-3320-4ca1-9612-fab17bde793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import re \n",
    "import seaborn as sns \n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Dropout, Embedding, BatchNormalization \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.utils import pad_sequences \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d60761-d561-484f-a77b-ba4d495d7398",
   "metadata": {},
   "source": [
    "### Step 2: Loading the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298ab26-7a8d-4fa3-bb6c-397b69ace777",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('AmazonReview.csv') \n",
    "\n",
    "# Printing shape of the dataset \n",
    "print(data.shape) \n",
    "# printing columns and rows information \n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20dd4c8-1f76-439f-9e95-bfbe007bc0c1",
   "metadata": {},
   "source": [
    "# Step 3: Preprocessing data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a045cc4-6173-4448-851f-6d4d75d688f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for NULL values \n",
    "print(\"Null Values:\\n\", data.isna().sum()) \n",
    "\n",
    "# dropping null values \n",
    "data = data.dropna() \n",
    "\n",
    "# again checking for NULL values \n",
    "print(\"Null Values after dropping:\\n\", data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7897e1f-efbd-4514-b99b-4baa5710b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of unique values in Sentiment column \n",
    "data['Sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d98899c-f888-4c5f-a612-0d27acc93179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  \n",
    "nltk.download('punkt_tab') \n",
    "\n",
    "\n",
    "print(nltk.data.find('tokenizers/punkt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d179860-b07c-4b78-a9eb-4b1a32d41597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean reviews\n",
    "def clean_reviews(text): \n",
    "    if isinstance(text, str):  # Check if the text is a string\n",
    "        text = text.lower()  # Convert to lower case\n",
    "        text = word_tokenize(text)  # Tokenization of words\n",
    "        text = [word for word in text if word not in stop_words]  # Stop words removal\n",
    "    else:\n",
    "        text = []  # Handle non-string or NaN values as empty list\n",
    "    return text \n",
    "\n",
    "# Handle non-string values in the 'Review' column\n",
    "data['Review'] = data['Review'].astype(str)  # Convert all to strings\n",
    "data['Review'] = data['Review'].fillna('')   # Fill NaN values with empty strings\n",
    "\n",
    "# Apply the cleaning function\n",
    "data['Review'] = data['Review'].apply(clean_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef2e58-8674-4e49-b16b-019fa619675f",
   "metadata": {},
   "source": [
    "### Step 4: Tokenization & Text Encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea06c4-7eea-4483-8d3a-4cf1b7ed7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500) (25000, 5)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer() \n",
    "\n",
    "# Convert all the reviews to a list \n",
    "reviews_to_list = data['Review'].tolist() \n",
    "tokenizer.fit_on_texts(reviews_to_list) \n",
    "\n",
    "# Generate text sequences \n",
    "text_sequences = tokenizer.texts_to_sequences(reviews_to_list)\n",
    "\n",
    "# Set the maximum number of words in a sequence\n",
    "max_words = 500\n",
    "\n",
    "# Padding sequences to ensure they all have the same length\n",
    "X = pad_sequences(text_sequences, maxlen=max_words)\n",
    "\n",
    "# One hot encoding the Sentiment column\n",
    "data = pd.get_dummies(data, columns=['Sentiment'])\n",
    "\n",
    "# Generate the output labels y\n",
    "y = data[['Sentiment_1', 'Sentiment_2', 'Sentiment_3', 'Sentiment_4', 'Sentiment_5']] \n",
    "\n",
    "# Print the shapes of X and y\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd6a69d-b8f8-4ce8-a7c2-7aed9c91881a",
   "metadata": {},
   "source": [
    "### Step 5: Train-Test Split (80% - 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecbf2db-2454-4c78-a71b-92f13e67e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86704646-0568-4cbb-be99-37da5190c864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 500) (5000, 500) (20000, 5) (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "reviews_to_list = data['Review'].tolist()\n",
    "tokenizer.fit_on_texts(reviews_to_list)\n",
    "text_sequences = tokenizer.texts_to_sequences(reviews_to_list)\n",
    "\n",
    "# Padding sequences\n",
    "max_words = 500\n",
    "X = pad_sequences(text_sequences, maxlen=max_words)\n",
    "\n",
    "# Generate the output labels y\n",
    "y = data[['Sentiment_1', 'Sentiment_2', 'Sentiment_3', 'Sentiment_4', 'Sentiment_5']]\n",
    "\n",
    "# Train Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5edf84-2110-4c88-8e03-6c8c6400454f",
   "metadata": {},
   "source": [
    "### Step 6: Model Building, Compiling and Training\n",
    "\n",
    "### 1. Build the Model using RNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f59c3fcb-44c4-4455-9665-e3ca6e467137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Simple_RNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Simple_RNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Creating a RNN model \n",
    "rnn = Sequential(name=\"Simple_RNN\") \n",
    "rnn.add(Embedding(len(tokenizer.word_index)+1, \n",
    "\t\t\t\t\t\tmax_words, \n",
    "\t\t\t\t\t\tinput_length=max_words)) \n",
    "\n",
    "rnn.add(SimpleRNN(128,activation='relu',return_sequences=True)) \n",
    "\n",
    "rnn.add(SimpleRNN(64,activation='relu',return_sequences=False)) \n",
    "\n",
    "rnn.add(Dense(5, activation='softmax')) \n",
    "\n",
    "# printing model summary \n",
    "print(rnn.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877822e-04f0-4eaf-a0f8-26a713058632",
   "metadata": {},
   "source": [
    "### 2. Compiling , Traning and Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd45c6d-b97b-4361-b183-31260f905258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model \n",
    "rnn.compile( \n",
    "\tloss=\"categorical_crossentropy\", \n",
    "\toptimizer='adam', \n",
    "\tmetrics=['accuracy'] \n",
    ") \n",
    "\n",
    "# Training the model \n",
    "history = rnn.fit(X_train, y_train, \n",
    "\t\t\t\t\t\tbatch_size=64, \n",
    "\t\t\t\t\t\tepochs=2, \n",
    "\t\t\t\t\t\tverbose=1, \n",
    "\t\t\t\t\t\tvalidation_data = (X_test, y_test)) \n",
    "\n",
    "# Printing model score on test data \n",
    "print(\"Score :\", rnn.evaluate(X_test, y_test, verbose=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0929e-5d1e-4180-8af0-75c579747835",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8d7cd",
   "metadata": {},
   "source": [
    "### Testing the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb6d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review_rating(text):\n",
    "    # Tokenize the input text\n",
    "    text_sequences_test = tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    # Pad the sequence to ensure it matches the input length expected by the model\n",
    "    testing = pad_sequences(text_sequences_test, maxlen=max_words)\n",
    "    \n",
    "    # Predict the rating (output class) using the trained model\n",
    "    y_pred_test = np.argmax(model.predict(testing), axis=1)\n",
    "    \n",
    "    # Return the predicted rating\n",
    "    return y_pred_test[0] + 1\n",
    "\n",
    "# Testing the prediction function\n",
    "rating1 = predict_review_rating('Worst product')\n",
    "print(\"The rating according to the review is: \", rating1)\n",
    "\n",
    "rating2 = predict_review_rating('Awesome product, I will recommend this to other users.')\n",
    "print(\"The rating according to the review is: \", rating2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
